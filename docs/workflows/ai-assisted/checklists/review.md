# Review Checklist (AI-Assisted Workflow)

Inspired by *AI Assisted Coding: Quicker Code Doesn’t Mean Higher Velocity*.

## Before Requesting Review
- [ ] Feature brief is attached and scoped to a single logical change.
- [ ] Diffs are split/annotated (≤150 LOC chunks) — include armchr notes or manual summaries.
- [ ] Tests/linters listed in the brief all ran locally with results noted.
- [ ] Documentation/session scripts updated if behavior changed.
- [ ] Verified that assistant-written tests actually fail when the code is broken (spot-check one test by mutating the code).

## During Review (Author + Reviewer together)
- [ ] Walk through each chunk, restating intent to confirm shared understanding.
- [ ] Trace critical paths manually (no relying solely on autogenerated tests).
- [ ] Confirm there’s no hidden hard-coding or silent error handling added by the assistant.
- [ ] Ensure both author and reviewer can explain the change without looking at the diff — if not, add annotations/cleanup.
- [ ] Treat the code like an external contribution: assume bugs until proven otherwise.

## Before Merge
- [ ] Reviewer signs off that maintainability is acceptable (naming, duplication, logging).
- [ ] Checklist items copied into the PR template with `[x]` marks.
- [ ] Verification commands pasted (or referenced) with timestamps.
- [ ] Follow-up briefs filed for anything deferred.
